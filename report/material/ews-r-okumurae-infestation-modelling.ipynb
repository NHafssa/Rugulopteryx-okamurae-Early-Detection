{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPyJJBjEUFM6HCJxqiAhtd5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kgEVQyk3-WuY","executionInfo":{"status":"ok","timestamp":1762344198124,"user_tz":0,"elapsed":19692,"user":{"displayName":"Aditi Paretkar","userId":"02646541838850375964"}},"outputId":"de4158ef-949e-4eeb-e901-fbee38923b4c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Generated 50/1000 samples\n","Generated 100/1000 samples\n","Generated 150/1000 samples\n","Generated 200/1000 samples\n","Generated 250/1000 samples\n","Generated 300/1000 samples\n","Generated 350/1000 samples\n","Generated 400/1000 samples\n","Generated 450/1000 samples\n","Generated 500/1000 samples\n","Generated 550/1000 samples\n","Generated 600/1000 samples\n","Generated 650/1000 samples\n","Generated 700/1000 samples\n","Generated 750/1000 samples\n","Generated 800/1000 samples\n","Generated 850/1000 samples\n","Generated 900/1000 samples\n","Generated 950/1000 samples\n","Generated 1000/1000 samples\n","\n","✅ Dataset created at: /content/infestation_dataset\n"]}],"source":["#sample data creation script\n","\n","import numpy as np\n","import imageio.v2 as imageio\n","from scipy.ndimage import binary_dilation\n","import os\n","\n","def generate_infestation_sequence(\n","    size=128, timesteps=10, n_seeds=3, spread_prob=0.3\n","):\n","    \"\"\"\n","    Generates a sequence of binary masks simulating algae infestation over time.\n","    Returns a NumPy array of shape (timesteps, size, size)\n","    \"\"\"\n","    rng = np.random.default_rng()\n","    masks = []\n","\n","    # initialize empty grid\n","    mask = np.zeros((size, size), dtype=np.uint8)\n","\n","    # random starting seeds\n","    seed_coords = rng.integers(0, size, (n_seeds, 2))\n","    for (x, y) in seed_coords:\n","        mask[x, y] = 1\n","\n","    # 3x3 neighborhood kernel\n","    structure = np.ones((3, 3), dtype=bool)\n","\n","    for _ in range(timesteps):\n","        masks.append(mask.copy())\n","\n","        # grow probabilistically\n","        dilated = binary_dilation(mask, structure=structure)\n","        new_growth = (dilated & (mask == 0)) & (rng.random(mask.shape) < spread_prob)\n","        mask = mask | new_growth\n","\n","    return np.stack(masks)\n","\n","\n","def generate_dataset(\n","    output_dir=\"infestation_dataset\",\n","    n_samples=1000,\n","    size=128,\n","    timesteps=10,\n","    n_seeds_range=(2, 5),\n","    spread_prob_range=(0.2, 0.4),\n","    save_as_images=True\n","):\n","    \"\"\"\n","    Generates a dataset of infestation masks and saves them to disk.\n","    Each sample folder contains a time series of infestation masks.\n","    \"\"\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    rng = np.random.default_rng()\n","\n","    for i in range(n_samples):\n","        n_seeds = rng.integers(*n_seeds_range)\n","        spread_prob = rng.uniform(*spread_prob_range)\n","\n","        masks = generate_infestation_sequence(\n","            size=size,\n","            timesteps=timesteps,\n","            n_seeds=n_seeds,\n","            spread_prob=spread_prob\n","        )\n","\n","        sample_dir = os.path.join(output_dir, f\"sample_{i:04d}\")\n","        os.makedirs(sample_dir, exist_ok=True)\n","\n","        if save_as_images:\n","            for t, mask in enumerate(masks):\n","                imageio.imwrite(\n","                    os.path.join(sample_dir, f\"mask_{t:02d}.png\"),\n","                    (mask * 255).astype(np.uint8)\n","                )\n","        else:\n","            np.save(os.path.join(sample_dir, \"masks.npy\"), masks)\n","\n","        if (i + 1) % 50 == 0:\n","            print(f\"Generated {i+1}/{n_samples} samples\")\n","\n","    print(f\"\\n✅ Dataset created at: {os.path.abspath(output_dir)}\")\n","\n","\n","# --- Run the generator ---\n","generate_dataset(\n","    output_dir=\"infestation_dataset\",\n","    n_samples=1000,    # total samples\n","    size=128,          # resolution\n","    timesteps=10,      # frames per sample\n","    n_seeds_range=(2, 5),\n","    spread_prob_range=(0.2, 0.4),\n","    save_as_images=True\n",")\n"]},{"cell_type":"code","source":["#this creates and uses the data\n","\"\"\"\n","infestation_unet_train.py\n","\n","Requirements:\n","- Python 3.8+\n","- numpy\n","- scipy\n","- imageio\n","- matplotlib (optional, for plotting)\n","- torch (PyTorch)\n","- torchvision\n","- scikit-learn\n","\n","Run:\n","    python infestation_unet_train.py\n","\"\"\"\n","\n","import os\n","import random\n","import numpy as np\n","from pathlib import Path\n","from scipy.ndimage import binary_dilation\n","import imageio.v2 as imageio\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import Adam\n","\n","# -------------------------\n","# 1) Synthetic generator\n","# -------------------------\n","def generate_infestation_sequence(size=128, timesteps=6, n_seeds=3, spread_prob=0.3, rng=None):\n","    \"\"\"\n","    Returns array shape (timesteps, H, W) with binary masks (0/1).\n","    \"\"\"\n","    if rng is None:\n","        rng = np.random.default_rng()\n","    masks = []\n","    mask = np.zeros((size, size), dtype=np.uint8)\n","\n","    seed_coords = rng.integers(0, size, (n_seeds, 2))\n","    for (x, y) in seed_coords:\n","        mask[x, y] = 1\n","\n","    structure = np.ones((3, 3), dtype=bool)\n","\n","    for _ in range(timesteps):\n","        masks.append(mask.copy())\n","        dilated = binary_dilation(mask, structure=structure)\n","        new_growth = (dilated & (mask == 0)) & (rng.random(mask.shape) < spread_prob)\n","        mask = mask | new_growth\n","\n","    return np.stack(masks)  # (timesteps, H, W)\n","\n","\n","def generate_dataset_in_memory(n_sequences=1000, size=128, timesteps=6,\n","                               n_seeds_range=(1,4), spread_prob_range=(0.15,0.45), seed=42):\n","    \"\"\"\n","    Generates sequences and returns tuple (X_pairs, Y_pairs)\n","    where each pair is (mask_t, mask_t+1)\n","    X_pairs shape: (N_pairs, 1, H, W)\n","    Y_pairs shape: (N_pairs, 1, H, W)\n","    \"\"\"\n","    rng = np.random.default_rng(seed)\n","    X_list = []\n","    Y_list = []\n","\n","    for i in range(n_sequences):\n","        n_seeds = int(rng.integers(n_seeds_range[0], n_seeds_range[1]+1))\n","        spread_prob = float(rng.uniform(spread_prob_range[0], spread_prob_range[1]))\n","        seq = generate_infestation_sequence(size=size, timesteps=timesteps, n_seeds=n_seeds, spread_prob=spread_prob, rng=rng)\n","        # create pairs (t -> t+1)\n","        for t in range(seq.shape[0] - 1):\n","            X_list.append(seq[t:t+1].astype(np.float32))  # (1,H,W)\n","            Y_list.append(seq[t+1:t+2].astype(np.float32))\n","    X = np.stack(X_list)  # (N_pairs, 1, H, W)\n","    Y = np.stack(Y_list)\n","    return X, Y\n","\n","# -------------------------\n","# 2) PyTorch Dataset\n","# -------------------------\n","class InfestationPairsDataset(Dataset):\n","    def __init__(self, X, Y, transform=None):\n","        self.X = X\n","        self.Y = Y\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        x = self.X[idx]  # float32 1xHxW\n","        y = self.Y[idx]\n","        # Optionally add small noise / augmentation\n","        if self.transform:\n","            x, y = self.transform(x, y)\n","        return torch.from_numpy(x), torch.from_numpy(y)\n","\n","# -------------------------\n","# 3) Simple U-Net (small)\n","# -------------------------\n","class ConvBlock(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super().__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n","            nn.BatchNorm2d(out_ch),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n","            nn.BatchNorm2d(out_ch),\n","            nn.ReLU(inplace=True)\n","        )\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class UNetSmall(nn.Module):\n","    def __init__(self, in_ch=1, out_ch=1, base_ch=32):\n","        super().__init__()\n","        self.enc1 = ConvBlock(in_ch, base_ch)\n","        self.pool = nn.MaxPool2d(2)\n","        self.enc2 = ConvBlock(base_ch, base_ch*2)\n","        self.enc3 = ConvBlock(base_ch*2, base_ch*4)\n","\n","        self.bottleneck = ConvBlock(base_ch*4, base_ch*8)\n","\n","        self.up3 = nn.ConvTranspose2d(base_ch*8, base_ch*4, kernel_size=2, stride=2)\n","        self.dec3 = ConvBlock(base_ch*8, base_ch*4)\n","        self.up2 = nn.ConvTranspose2d(base_ch*4, base_ch*2, kernel_size=2, stride=2)\n","        self.dec2 = ConvBlock(base_ch*4, base_ch*2)\n","        self.up1 = nn.ConvTranspose2d(base_ch*2, base_ch, kernel_size=2, stride=2)\n","        self.dec1 = ConvBlock(base_ch*2, base_ch)\n","\n","        self.head = nn.Conv2d(base_ch, out_ch, kernel_size=1)\n","\n","    def forward(self, x):\n","        e1 = self.enc1(x)\n","        e2 = self.enc2(self.pool(e1))\n","        e3 = self.enc3(self.pool(e2))\n","        b = self.bottleneck(self.pool(e3))\n","        d3 = self.up3(b)\n","        d3 = torch.cat([d3, e3], dim=1)\n","        d3 = self.dec3(d3)\n","        d2 = self.up2(d3)\n","        d2 = torch.cat([d2, e2], dim=1)\n","        d2 = self.dec2(d2)\n","        d1 = self.up1(d2)\n","        d1 = torch.cat([d1, e1], dim=1)\n","        d1 = self.dec1(d1)\n","        out = self.head(d1)\n","        return out  # logits\n","\n","# -------------------------\n","# 4) Metrics\n","# -------------------------\n","def iou_score(preds, targets, threshold=0.5, eps=1e-7):\n","    \"\"\"\n","    preds: torch tensor logits or probabilities (B,1,H,W)\n","    targets: tensor (B,1,H,W)\n","    \"\"\"\n","    if preds.dtype != torch.uint8 and preds.dtype != torch.bool:\n","        probs = torch.sigmoid(preds)\n","        preds_bin = (probs > threshold).float()\n","    else:\n","        preds_bin = preds.float()\n","    inter = (preds_bin * targets).sum(dim=(1,2,3))\n","    union = ((preds_bin + targets) >= 1).float().sum(dim=(1,2,3))\n","    iou = (inter + eps) / (union + eps)\n","    return iou.mean().item()\n","\n","# -------------------------\n","# 5) Training loop\n","# -------------------------\n","def train_model(model, train_loader, val_loader, device, epochs=10, lr=1e-3, save_dir=\"checkpoints\"):\n","    opt = Adam(model.parameters(), lr=lr)\n","    criterion = nn.BCEWithLogitsLoss()\n","    os.makedirs(save_dir, exist_ok=True)\n","\n","    best_val_iou = 0.0\n","    for epoch in range(1, epochs+1):\n","        model.train()\n","        train_loss = 0.0\n","        for xb, yb in train_loader:\n","            xb = xb.to(device)\n","            yb = yb.to(device)\n","            logits = model(xb)\n","            loss = criterion(logits, yb)\n","            opt.zero_grad()\n","            loss.backward()\n","            opt.step()\n","            train_loss += loss.item() * xb.size(0)\n","        train_loss /= len(train_loader.dataset)\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0.0\n","        iou_acc = 0.0\n","        with torch.no_grad():\n","            for xb, yb in val_loader:\n","                xb = xb.to(device)\n","                yb = yb.to(device)\n","                logits = model(xb)\n","                loss = criterion(logits, yb)\n","                val_loss += loss.item() * xb.size(0)\n","                iou_acc += iou_score(logits.detach().cpu(), yb.detach().cpu()) * xb.size(0)\n","        val_loss /= len(val_loader.dataset)\n","        val_iou = iou_acc / len(val_loader.dataset)\n","\n","        print(f\"Epoch {epoch}/{epochs} — train_loss: {train_loss:.4f} val_loss: {val_loss:.4f} val_iou: {val_iou:.4f}\")\n","\n","        # Save best\n","        if val_iou > best_val_iou:\n","            best_val_iou = val_iou\n","            torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))\n","    print(\"Training finished. Best val IoU:\", best_val_iou)\n","\n","# -------------------------\n","# 6) Utilities: visualize some examples\n","# -------------------------\n","def save_predictions_sample(model, dataset, device, out_dir=\"pred_samples\", n_samples=8):\n","    os.makedirs(out_dir, exist_ok=True)\n","    model.eval()\n","    with torch.no_grad():\n","        for i in range(n_samples):\n","            xb, yb = dataset[i]\n","            xb_t = xb.unsqueeze(0).to(device)  # add batch\n","            logits = model(xb_t)\n","            probs = torch.sigmoid(logits).squeeze().cpu().numpy()\n","            pred_bin = (probs > 0.5).astype(np.uint8)\n","            input_mask = xb.squeeze().numpy().astype(np.uint8)\n","            true_mask = yb.squeeze().numpy().astype(np.uint8)\n","\n","            # Compose image: left=input, middle=truth, right=pred\n","            H, W = input_mask.shape\n","            canvas = np.zeros((H, W*3), dtype=np.uint8)\n","            canvas[:, :W] = input_mask * 255\n","            canvas[:, W:2*W] = true_mask * 255\n","            canvas[:, 2*W:3*W] = pred_bin * 255\n","            imageio.imwrite(os.path.join(out_dir, f\"sample_{i:02d}.png\"), canvas)\n","\n","    print(\"Saved sample predictions to\", out_dir)\n","\n","# -------------------------\n","# 7) Main: generate data, train, eval\n","# -------------------------\n","def main():\n","    # SETTINGS - adjust as needed\n","    IMG_SIZE = 128\n","    SEQS = 250           # number of sequences (each has timesteps frames)\n","    TIMESTEPS = 6\n","    BATCH_SIZE = 32\n","    EPOCHS = 12\n","    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(\"Using device:\", DEVICE)\n","\n","    # 1) Generate dataset (in memory)\n","    print(\"Generating synthetic dataset ...\")\n","    X, Y = generate_dataset_in_memory(n_sequences=SEQS, size=IMG_SIZE, timesteps=TIMESTEPS,\n","                                      n_seeds_range=(1,4), spread_prob_range=(0.12,0.45), seed=123)\n","    # X,Y shapes: (N_pairs, 1, H, W)\n","    print(\"Total pairs:\", X.shape[0])\n","\n","    # 2) Split into train/val\n","    X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.12, random_state=42)\n","    train_ds = InfestationPairsDataset(X_train, Y_train)\n","    val_ds = InfestationPairsDataset(X_val, Y_val)\n","    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n","    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n","\n","    # 3) Model\n","    model = UNetSmall(in_ch=1, out_ch=1, base_ch=32).to(DEVICE)\n","\n","    # 4) Train\n","    train_model(model, train_loader, val_loader, device=DEVICE, epochs=EPOCHS, lr=1e-3, save_dir=\"checkpoints\")\n","\n","    # 5) Save some predictions\n","    save_predictions_sample(model, val_ds, device=DEVICE, out_dir=\"pred_samples\", n_samples=12)\n","\n","if __name__ == \"__main__\":\n","    main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NKUHy07tB2yR","executionInfo":{"status":"ok","timestamp":1762340647822,"user_tz":0,"elapsed":67867,"user":{"displayName":"Aditi Paretkar","userId":"02646541838850375964"}},"outputId":"4999e15e-55c0-4aa8-ad70-ed0f326f79a5"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Generating synthetic dataset ...\n","Total pairs: 1250\n","Epoch 1/12 — train_loss: 0.3587 val_loss: 0.2337 val_iou: 0.2220\n","Epoch 2/12 — train_loss: 0.1999 val_loss: 0.1630 val_iou: 0.5082\n","Epoch 3/12 — train_loss: 0.1255 val_loss: 0.1047 val_iou: 0.5352\n","Epoch 4/12 — train_loss: 0.0810 val_loss: 0.0642 val_iou: 0.5358\n","Epoch 5/12 — train_loss: 0.0549 val_loss: 0.0465 val_iou: 0.5379\n","Epoch 6/12 — train_loss: 0.0391 val_loss: 0.0349 val_iou: 0.5403\n","Epoch 7/12 — train_loss: 0.0291 val_loss: 0.0253 val_iou: 0.5408\n","Epoch 8/12 — train_loss: 0.0226 val_loss: 0.0204 val_iou: 0.5385\n","Epoch 9/12 — train_loss: 0.0181 val_loss: 0.0162 val_iou: 0.5391\n","Epoch 10/12 — train_loss: 0.0149 val_loss: 0.0135 val_iou: 0.5364\n","Epoch 11/12 — train_loss: 0.0126 val_loss: 0.0115 val_iou: 0.5394\n","Epoch 12/12 — train_loss: 0.0108 val_loss: 0.0099 val_iou: 0.5386\n","Training finished. Best val IoU: 0.5407860430081686\n","Saved sample predictions to pred_samples\n"]}]}]}